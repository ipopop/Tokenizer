# Tokenizer
Apply Tokenization Techniques with Python3

## Todo :

> Stopwords, tokenizers, and word clouds may seem simple to implement, but the devil is in the details when working on a text. Itâ€™s important to get some practice with these powerful tools.

Here are some steps to follow:

1. Find a 'Wikipedia' page, a text from 'Project Gutenberg', or any other NLP dataset.

2. Tokenize the text using 'NLTK  WordPunctTokenizer'.

3. Explore the list of 'tokens' and their frequency.

4.  Experiment with the 'WordCloud()' parameters to generate different word clouds from the original text:
    - collocations = False
    - normalize_plurals = True or False
    - include_numbers = True or False
    - min_word_length
    - stopwords

5. Remove 'stopwords' from the original text.

6.  Use 'string.punctuation' and  'string.digits' to remove punctuation and numbers.