# Tokenizer
Apply Tokenization Techniques with Python3

## Todo :

Stopwords, tokenizers, and word clouds may seem simple to implement, but the devil is in the details when working on a text. Itâ€™s important to get some practice with these powerful tools.

Here are some steps to follow:

    Find a Wikipedia page, a text from Project Gutenberg, or any other NLP dataset.

    Tokenize the text using NLTK  WordPunctTokenizer  .

    Explore the list of tokens and their frequency.

    Experiment with the  WordCloud()  parameters to generate different word clouds from the original text:

        collocations = False

        normalize_plurals = True or False

        include_numbers = True or False

        min_word_length

        stopwords

    Remove stopwords from the original text.

    Use  string.punctuation  and  string.digits  to remove punctuation and numbers.